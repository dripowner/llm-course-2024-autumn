{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvhPa7a59AIG"
   },
   "source": [
    "<font color=red>**Danger zone:**</font> you'll be fine-tuning a model to generate positive, negative or even toxic reviews. We'll be doing this for fun, but this is also the technique for [review bombing](https://en.wikipedia.org/wiki/Review_bomb), bot farms on social media and other less than dignified stuff. It is ultimately your decision how you apply this knowledge, but before you choose, ask yourself: is this why you chose to learn ML?\n",
    "\n",
    "\n",
    "# LLMs Alignment with Reinforcement Learning from human feedback (RLHF).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgfL4bSSAXan"
   },
   "source": [
    "In this homework, you're gonna fine-tune a language model with reinforcement learning to make it generat bad (or good) reviews.\n",
    "\n",
    "To perform RL-based fine-tuning, we'll use a new (in this course) library called [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl). TRL implements the main reinforcement learning components of RLHF: reward modeling and fine-tuning with PPO.\n",
    "\n",
    "![img](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/TRL-readme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cJfrTbFYAx8"
   },
   "source": [
    "## Stage 0: load model\n",
    "\n",
    "To see how TRL works, we'll use it to align GPT2 on IMDB dataset to generate negative movie reviews. In fact, __it's your choice whether you want positive or negative reviews__, however I recommend you to focus on negative ones, in order to see greater effect after RLHF\n",
    "\n",
    "But before you choose, let's take a look at the baseline model: a GPT-2 fine-tuned on generating arbitrary movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHs22MXdPify",
    "outputId": "a95af581-560b-4845-e202-a84150940196"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\trl\\trainer\\ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yxtag\\.cache\\huggingface\\hub\\models--lvwerra--gpt2-imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\transformers\\modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import trl\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "main_tokenizer = transformers.AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
    "main_model = transformers.AutoModelForCausalLM.from_pretrained(\"lvwerra/gpt2-imdb\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KE3jo7uhQrvK",
    "outputId": "b937058f-6014-4180-f5e5-b09ed4a2285a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text: The movie is as I've seen it, if you can believe what I've heard in comments made recently, it is a complete failure. I wish that this movie had never been made. I think this was a great example to the world of acting that I\n"
     ]
    }
   ],
   "source": [
    "inputs = main_tokenizer(\"The movie\", return_tensors='pt').to(device)\n",
    "generated_ids = main_model.generate(**inputs, max_new_tokens=50, do_sample=True)\n",
    "print(\"\\nGenerated text:\", main_tokenizer.decode(generated_ids.flatten().cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJbfhMEpR4Sz"
   },
   "source": [
    "If you run this cell a couple of times, you'll see that the model generates both positive, negative and neutral reviews in some proportion. What we're gonna do next is teach the model to generate more positive (or negative) reviews.\n",
    "\n",
    "Similarly to InstructGPT, we're gonna do that in 2 stages:\n",
    "- **train a reward model** to assign higher values to positive (or negative) reviews\n",
    "- fine-tune the language model to **maximize that reward using [proximal policy optimization](https://openai.com/research/openai-baselines-ppo)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bcv4uC7xb26Z"
   },
   "source": [
    "## Stage 1: train a reward model\n",
    "\n",
    "First, we'll train a BERT-like model as our reward model. We'll generate a synthetic pairwise rankings to emulate human rankings.\n",
    "\n",
    "__Q:__ why do I need a reward model? Can I just use a pre-trained sentiment classifier? <br> __A:__ Yes, you can - but that only works for movie reviews. But this homework will teach you how to do RLHF for any kind objective.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WeOdZ_ayc9dy",
    "outputId": "3813d14b-d113-4154-a237-04bf3d5bbbe5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yxtag\\.cache\\huggingface\\hub\\models--distilbert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# We'll be fine-tuning a small BERT-like model for now. Please try other models for the main assignment.\n",
    "reward_model = transformers.AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", device_map=device)\n",
    "reward_tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZUUNQo-d11b"
   },
   "source": [
    "__Note that__ the reward model has a separate tokenizer, different from the main model. They don't need to be the same for RLHF fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDBPairwiseDataset(Dataset):\n",
    "\t\"\"\" \n",
    "\tA dataset of all possible pairs of chosen and rejected texts for TRL reward training format.\n",
    "\n",
    "\tThis dataset is designed to facilitate the training of a reward model by providing pairs of\n",
    "\ttexts where one is preferred (chosen) and the other is not (rejected). Each sample in the dataset\n",
    "\tis a dictionary containing tokenized input IDs and attention masks for both the chosen and rejected\n",
    "\ttexts.\n",
    "\n",
    "\tParameters:\n",
    "\timdb: dataset to pairwise\n",
    "\ttokenizer: The tokenizer used to preprocess the texts\n",
    "\taccepted_label (int): The label that indicates a chosen text. Texts with this label are considered\n",
    "\t\t\t\t\t\t  preferred, while others are considered rejected.\n",
    "\n",
    "\tMethods:\n",
    "\t__len__(): Returns the total number of possible pairs of chosen and rejected texts.\n",
    "\t__getitem__(index): Returns a dictionary containing tokenized inputs for a specific pair of chosen\n",
    "\t\t\t\t\t\tand rejected texts.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdef __init__(self, imdb, tokenizer, accepted_label):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.chosen_texts = imdb.filter(lambda x: x[\"label\"] == accepted_label)[\"text\"]\n",
    "\t\tself.rejected_texts = imdb.filter(lambda x: x[\"label\"] != accepted_label)[\"text\"]\n",
    "\n",
    "\t\tassert self.chosen_texts, f\"no texts with label {accepted_label}\"\n",
    "\t\t# print(f\"Found {len(self.chosen_texts)} chosen and {len(self.rejected_texts)} rejected texts, {len(self)} pairs\")\n",
    "\n",
    "\t\tself.column_names = [\n",
    "\t\t\t'input_ids_chosen', 'attention_mask_chosen',\n",
    "\t\t\t'input_ids_rejected', 'attention_mask_rejected'\n",
    "\t\t]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.chosen_texts) * len(self.rejected_texts)  # all pairs\n",
    "\n",
    "\tdef __getitem__(self, index: int):\n",
    "\t\tchoosen_text = self.chosen_texts[(index - 1) // len(self.rejected_texts)]\n",
    "\t\trejected_text = self.rejected_texts[(index - 1) % len(self.rejected_texts)]\n",
    "\n",
    "\t\tchosen_inputs = self.tokenizer(choosen_text, return_tensors='pt', truncation=True)\n",
    "\t\trejected_inputs = self.tokenizer(rejected_text, return_tensors='pt', truncation=True)\n",
    "\t\t\n",
    "\t\treturn dict(\n",
    "\t\t\tinput_ids_chosen=chosen_inputs['input_ids'][0],\n",
    "\t\t\tattention_mask_chosen=chosen_inputs['attention_mask'][0],\n",
    "\t\t\tinput_ids_rejected=rejected_inputs['input_ids'][0],\n",
    "\t\t\tattention_mask_rejected=rejected_inputs['attention_mask'][0],\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olo-bvgNcwEC",
    "outputId": "21b2a6f2-03cf-47a8-ffd0-c43a85ca331e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHOSEN: [CLS] If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story. < br / > < br / > One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives ( unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film ). < br / > < br / > One might better spend one's time staring out a window at a tree growing. < br / > < br / > [SEP]\n",
      "REJECTED: [CLS] Well, I come from Bulgaria where it's almost impossible to have a tornado but my imagination tells me to be \" very, very afraid \"!!! This guy ( Devon Sawa ) has done a great job with this movie! I don't know exactly how old he was but he didn't act like a child ( WELL DONE )! Now about the tornado - it wasn't very realistic but frightens you! If you want to have a nice time in front of the telly - this is the movie! [SEP]\n"
     ]
    }
   ],
   "source": [
    "TARGET_LABEL = 0 # negative reviews\n",
    "imdb = datasets.load_dataset(\"imdb\", split='train')\n",
    "reward_data = IMDBPairwiseDataset(imdb, reward_tokenizer, accepted_label=TARGET_LABEL)\n",
    "\n",
    "sample = reward_data[31337]\n",
    "print('CHOSEN:', reward_tokenizer.decode(sample['input_ids_chosen']))\n",
    "print('REJECTED:', reward_tokenizer.decode(sample['input_ids_rejected']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZRczyofiSl0"
   },
   "source": [
    "We'll be using `trl.RewardTrainer` - a special case of `transformers.Trainer`.\n",
    "\n",
    "![img](https://i.imgur.com/2JzNAPs.png)\n",
    "\n",
    "Note that the model itself does not score pairs: it processes chosen ($y_w$) and rejected ($y_l$) samples independently. To minimize this loss, the reward model needs to score chosen sample higher than the rejected one. Note that the formula also assumes some context $x$, which is useful for seq2seq tasks. In our case of movie reviews, $x$ is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oaQ_-JAzakJs",
    "outputId": "923b8692-5627-43bb-89b4-a42a7d0ff805"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\trl\\trainer\\reward_trainer.py:174: UserWarning: When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig. It will be set to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\trl\\trainer\\reward_trainer.py:191: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\accelerate\\accelerator.py:439: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "  5%|▌         | 50/1000 [00:41<10:52,  1.45it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5332, 'learning_rate': 1.34232e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100/1000 [01:16<12:16,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1951, 'learning_rate': 1.2718200000000001e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 150/1000 [01:50<09:40,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1565, 'learning_rate': 1.20132e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 200/1000 [02:24<09:08,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1237, 'learning_rate': 1.1308200000000001e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 250/1000 [02:59<08:32,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.103, 'learning_rate': 1.06032e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 300/1000 [03:33<07:59,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1051, 'learning_rate': 9.8982e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 350/1000 [04:07<07:24,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0977, 'learning_rate': 9.1932e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 400/1000 [04:41<06:51,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0822, 'learning_rate': 8.4882e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 450/1000 [05:16<06:15,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0722, 'learning_rate': 7.7832e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 500/1000 [05:50<05:42,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0827, 'learning_rate': 7.0782e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      " 55%|█████▌    | 550/1000 [06:26<05:08,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0799, 'learning_rate': 6.3732e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 600/1000 [07:00<04:34,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0831, 'learning_rate': 5.6682e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 650/1000 [07:34<03:59,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0706, 'learning_rate': 4.9632e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 700/1000 [08:09<03:25,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0586, 'learning_rate': 4.2582e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 750/1000 [08:43<02:52,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0722, 'learning_rate': 3.5532e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 800/1000 [09:17<02:17,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0816, 'learning_rate': 2.8482e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 850/1000 [09:52<01:42,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0648, 'learning_rate': 2.1432e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 900/1000 [10:26<01:08,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0619, 'learning_rate': 1.4382e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 950/1000 [11:00<00:33,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0651, 'learning_rate': 7.332e-07, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:34<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0573, 'learning_rate': 2.82e-08, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:37<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 697.7943, 'train_samples_per_second': 45.859, 'train_steps_per_second': 1.433, 'train_loss': 0.11233135890960694, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.11233135890960694, metrics={'train_runtime': 697.7943, 'train_samples_per_second': 45.859, 'train_steps_per_second': 1.433, 'train_loss': 0.11233135890960694, 'epoch': 0.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = trl.RewardConfig(  # like transformers.TrainingArguments\n",
    "\toutput_dir=\"reward_model\",\n",
    "\tper_device_train_batch_size=32,\n",
    "\tgradient_accumulation_steps=1,\n",
    "\tlearning_rate=1.41e-5,\n",
    "\tmax_steps=1_000,              # note: training may need more than 1k steps\n",
    "\tlogging_steps=50,\n",
    "\tgradient_checkpointing=True,  # reduce memory usage but train ~30% slower\n",
    "\tgradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "\tfp16=True,                    # disable this on CPU or on very old GPUs\n",
    "\treport_to='none',\n",
    "\t# you may add any other hyperparameters that you found useful\n",
    ")\n",
    "\n",
    "trainer = trl.RewardTrainer(\n",
    "\tmodel=reward_model,\n",
    "\targs=training_args,\n",
    "\ttokenizer=reward_tokenizer,\n",
    "\ttrain_dataset=reward_data,\n",
    "\tpeft_config=None,  # optionally, you may tune with LoRA, prompt-tuning, etc\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRk7z-2r4C-A",
    "outputId": "29e3b246-2adb-414f-8726-e6060ddaa56a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model.gradient_checkpointing_disable()\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZIaS-gRo8yc"
   },
   "source": [
    "### Sanity-check the reward model\n",
    "\n",
    "Let's check how our reward model performs.\n",
    "\n",
    "__Your task__ is to measure how often does your reward model can rank a pair of (chosen and rejected) reviews correctly. Please measure this separately for train data (`imdb`) and a separate test set loaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeQ108nOZ7nO",
    "outputId": "576a2031-f1fc-49cc-b470-c9fd4983ffbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: This movie sucked. It really was a waste of my life. The acting was atrocious, the plot completely implausible. Long, long story short, these people get \"terrorized\" by this pathetic \"crazed killer\", but completely fail to fight back in any manner. And this is after they take a raft on a camping trip, with no gear, and show up at a campsite that is already assembled and completely stocked with food and clothes and the daughters headphones. Additionally, after their boat goes missing, they panic that they're stuck in the woods, but then the daughters boyfriend just shows up and they apparently never consider that they could just hike out of the woods like he did to get to them. Like I said, this movie sucks. A complete joke. Don't let your girlfriend talk you into watching it.\n",
      "REWARD: 5.078125\n",
      "LABEL: 0\n",
      "\n",
      "TEXT: Good: Engaging cinematic firefights, great presentation, vehicles are actually fun to drive, fairly appealing multiplayer, faithful to the movie, and the list goes on.<br /><br />Bad: Main missions are a bit short.<br /><br />This game defines what a \"good\" third person shooter(not necessarily a spy-game) is. Great firefights carry on the story and make you want to complete EVERY single mission through, and unlock all the genuine bonuses the game has to offer. The hype this game had, was lived up to, and I personally think you should buy it, and hook up with a couple of friends and play this one. Loads of fun. <br /><br />The sound in this game, is a rip-roaring achievement from a few previous bond games, and firing a weapon, really feels like you're firing a weapon. It ties in with the aspect that you are a deadly and ruthless spy.<br /><br />All in all, this game makes you excited and satisfied after you make it through, and some multiplayer that can compete with the standards of the crafty James Bond \"Nightfire\" game for gamecube.\n",
      "REWARD: -4.7890625\n",
      "LABEL: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sample_index in 45, 16000:\n",
    "\tprint('TEXT:', imdb[sample_index]['text'])\n",
    "\tinputs = reward_tokenizer(imdb[sample_index]['text'], truncation=True, return_tensors='pt').to(device)\n",
    "\twith torch.no_grad():\n",
    "\t\treward = reward_model(**inputs).logits[0, 0].item()\n",
    "\t\tprint(\"REWARD:\", reward)\n",
    "\tprint('LABEL:', imdb[sample_index]['label'])\n",
    "\tprint()\n",
    "\n",
    "# note: your reward model may produce different absolute rewards.\n",
    "# This is fine as long as the rewards are ordered correctly (most of the time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GITb0RF_FVUH"
   },
   "source": [
    "First of all, let's implement `compute_reward` function. Note that we use plaintext reviews because main model uses a different tokenizer from the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6eFIM_wlJrn"
   },
   "outputs": [],
   "source": [
    "from torch import Tensor, no_grad\n",
    "\n",
    "def compute_reward(reward_model, reward_tokenizer, texts: list[str], device='cpu') -> Tensor:\n",
    "\t\"\"\"\n",
    "\tCompute the reward scores for a list of texts using a specified reward model and tokenizer.\n",
    "\n",
    "\tParameters:\n",
    "\treward_model: The model used to compute the reward scores\n",
    "\treward_tokenizer: The tokenizer for reward_model\n",
    "\ttexts (list[str]): A list of text strings for which the reward scores are to be computed.\n",
    "\tdevice (str, optional): The device on which the computation should be performed. Default is 'cpu'.\n",
    "\n",
    "\tReturns:\n",
    "\ttorch.Tensor: A tensor containing the reward scores for each input text. The scores are extracted\n",
    "\t\t\t\t  from the logits of the reward model.\n",
    "\n",
    "\tExample:\n",
    "\t>>> compute_reward(my_reward_model, my_reward_tokenizer, [\"text1\", \"text2\"])\n",
    "\ttensor([ 5.1836, -4.8438], device='cpu')\n",
    "\t\"\"\"\n",
    "\t\n",
    "\treward_model.to(device)\n",
    "\tinputs = reward_tokenizer(texts, truncation=True, padding=True, return_tensors='pt').to(device)\n",
    "\t\n",
    "\twith no_grad():\n",
    "\t\treward = reward_model(**inputs).logits[:, 0]\n",
    "\t\n",
    "\treturn reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BhJ4WFjBhOMA",
    "outputId": "8a5c9990-dc7e-421b-af04-6871a3efa2fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.0742, -4.7891], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "rewards = compute_reward(reward_model, reward_tokenizer, [imdb[45]['text'], imdb[16000]['text']], device=device)\n",
    "print(rewards)\n",
    "assert rewards[0] > rewards[1]\n",
    "assert rewards[0] > 0\n",
    "assert rewards[1] < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "wzxa8k37mPS7"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def eval_reward_model(reward_model, reward_tokenizer, test_dataset, target_label, device='cpu'):\n",
    "\t\"\"\"\n",
    "\tEvaluate the performance of a reward model by comparing reward scores for chosen and rejected reviews. \n",
    "\n",
    "\tThis function selects reviews from a test dataset based on a target label and evaluates the reward model's\n",
    "\tability to assign higher scores to chosen reviews compared to rejected ones. The evaluation is performed\n",
    "\tin batches for efficiency.\n",
    "\tNote that reward scores are compared on corresponding chosen and rejected reviews: \n",
    "\t\tchosen_reviews[0] vs rejected_reviews[0], \n",
    "\t\tchosen_reviews[1] vs rejected_reviews[1],\n",
    "\t\tetc.\n",
    "\n",
    "\tParameters:\n",
    "\treward_model: The model used to compute the reward scores\n",
    "\treward_tokenizer: The tokenizer for reward_model\n",
    "\ttes_dataset: test Dataset\n",
    "\ttarget_label (0 or 1): The label used to select chosen reviews. Reviews with this label are considered chosen,\n",
    "\t\t\t\t  while others are considered rejected.\n",
    "\tdevice (str, optional): The device on which the computation should be performed. Default is 'cpu'.\n",
    "\n",
    "\tReturns:\n",
    "\tfloat: The accuracy of the reward model, calculated as the proportion of times the model assigns a higher\n",
    "\t\t   reward score to the chosen review compared to the rejected review.\n",
    "\n",
    "\tExample:\n",
    "\t>>> accuracy = eval_reward_model(my_reward_model, my_reward_tokenizer, test_data, target_label=1)\n",
    "\t>>> print(f\"Model accuracy: {accuracy:.2%}\")\n",
    "\t\"\"\"\n",
    "\n",
    "\tchosen_reviews = test_dataset.filter(lambda x: x['label'] == target_label)['text']\n",
    "\trejected_reviews = test_dataset.filter(lambda x: x['label'] != target_label)['text']\n",
    "\n",
    "\tif len(chosen_reviews) != len(rejected_reviews):\n",
    "\t\tmin_len = min(len(chosen_reviews), len(rejected_reviews))\n",
    "\t\tchosen_reviews = chosen_reviews[:min_len]\n",
    "\t\trejected_reviews = rejected_reviews[:min_len]\n",
    "\n",
    "\tassert len(chosen_reviews) == len(rejected_reviews)\n",
    "\n",
    "\tcorrect_answers_count = 0\n",
    "\tfor chosen_review, rejected_review in tqdm(zip(chosen_reviews, rejected_reviews)):\n",
    "\n",
    "\t\tchosen_ids = reward_tokenizer(chosen_review, truncation=True, padding=True, return_tensors='pt').to(device)\n",
    "\t\trejected_ids = reward_tokenizer(rejected_review, truncation=True, padding=True, return_tensors='pt').to(device)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tchosen_reward = reward_model(**chosen_ids).logits[0, 0].item()\n",
    "\t\t\trejected_reward = reward_model(**rejected_ids).logits[0, 0].item()\n",
    "\t\t\n",
    "\t\tif chosen_reward > rejected_reward:\n",
    "\t\t\tcorrect_answers_count += 1\n",
    "\n",
    "\treturn correct_answers_count / len(chosen_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "f70928bb66b24415ad03f49633775e64",
      "11b0022cfbaa488db010a2f22a17b7cc",
      "015524419b8a4a21bec9ca2462a1bdae",
      "351c280e3e0644e08478893632ca878c",
      "9123a18088fb454d915218f37f72a790",
      "3fd865cd993242cdad016d9794a75f6e",
      "f986e194b4944ead94190797f28015fe",
      "c3bb5755a3f44b519c4203430846e3d2",
      "62e010f8c15d402aba1e045de8e8a50b",
      "579edffceded4184ab0009cef5e8d87e",
      "83eac7e3fc554ea09c2a41452cad2cde"
     ]
    },
    "id": "1OvHokADapTh",
    "outputId": "9921947c-07c7-4821-a706-f6f945d4f08c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 320059.58 examples/s]\n",
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 411892.76 examples/s]\n",
      "12500it [02:48, 74.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.96928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_test = datasets.load_dataset(\"imdb\", split='test')\n",
    "\n",
    "test_accuracy = eval_reward_model(\n",
    "\treward_model,\n",
    "\treward_tokenizer,\n",
    "\timdb_test,\n",
    "\ttarget_label=TARGET_LABEL,\n",
    "\tdevice=device,\n",
    ")\n",
    "\n",
    "print('test accuracy: {}'.format(test_accuracy))\n",
    "assert test_accuracy > 0.94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHCWHMyRw2-k"
   },
   "source": [
    "### Reward-guided generation (1 point)\n",
    "\n",
    "If you did everything right, by now you should have a decent reward model. Before we use it for reinforcement learning, let's see if we can align model samples without any training.\n",
    "\n",
    "To do so, you can use reward-guided inference: __generate N=16 samples, then select the one with the highest reward__ (according to your reward model).\n",
    "\n",
    "For this problem, it's on you to demonstrate whether or not your code works. Find at least 5 neutral prompts such as \"This movie is\" (...), generate samples, rank them based on reward and show which samples get the highest reward.\n",
    "\n",
    "Note: it is faster to generate samples in parallel, rather than sequentially, as follows:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"This movie is\", \"The movie was\", \"I want to say that film\", \"Well, the movie was\", \"The movie was really\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_tokenizer.pad_token = main_tokenizer.eos_token\n",
    "main_tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BRsyb2cq5dR",
    "outputId": "af7df016-7656-47b9-9d7b-72bebb62c4b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: This movie is about a woman who accidentally kills her sister's dog. She is in love with a man with whom she dated for at first only 20 seconds. She wants revenge and she has an affair - but as a result of what happened to her, she breaks\n",
      "Sample: The movie was not exactly a movie. It's more like a play. At the end of a role played by Marv Levy, I actually laughed out loud. I was not sure if he was crying or laughing in that one scene. It could have been funny\n",
      "Sample: I want to say that film has been really great, though. The story was also good in certain aspects; for example, I love Cemetera, she is great in her role of the daughter of a wealthy French woman, but we have to be careful not to forget\n",
      "Sample: Well, the movie was okay enough (it stars the legendary \"Vladimir\") but I still thought it was just plain bad! Not to mention that no one who knew who The Master of Evil was was ever introduced to it in the theaters..and I was wrong!\n",
      "Sample: The movie was really good, as the movie didn't really have much to recommend. But I'll admit I was a bit surprised when I first saw it, and I had to see it to be sure.<br /><br />Although it had the look and feel\n"
     ]
    }
   ],
   "source": [
    "inputs = main_tokenizer(prompts, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "for candidate in main_model.generate(**inputs, max_new_tokens=50, do_sample=True):\n",
    "\tprint(\"Sample:\", main_tokenizer.decode(candidate.flatten().cpu().numpy().tolist(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "r08F4lz7yxE1"
   },
   "outputs": [],
   "source": [
    "def generate_with_reward_guidance(\n",
    "\t\tmain_model, main_tokenizer,\n",
    "\t\treward_model, reward_tokenizer,\n",
    "\t\tN=16,\n",
    "\t\tdevice='cpu',\n",
    "\t):\n",
    "\t\"\"\"\n",
    "\tGenerate text samples using a main model and select the best sample based on a reward model's guidance.\n",
    "\n",
    "\tThis function generates multiple text samples from a main model, evaluates each sample using a reward model,\n",
    "\tand returns the sample with the highest reward score. The process is guided by the reward model to select\n",
    "\tthe most desirable output.\n",
    "\n",
    "\tParameters:\n",
    "\tmain_model: The language model used to generate text samples.\n",
    "\tmain_tokenizer: The tokenizer for main_model\n",
    "\treward_model: The model used to compute reward scores for the generated samples.\n",
    "\treward_tokenizer: The tokenizer for reward_model\n",
    "\tN (int, optional): The number of text samples to generate. Default is 16.\n",
    "\tdevice (str, optional): The device on which the computation should be performed. Default is 'cpu'.\n",
    "\n",
    "\tReturns:\n",
    "\tstr: The generated text sample with the highest reward score.\n",
    "\t\"\"\"\n",
    "\tmain_model.to(device)\n",
    "\tgenerated_ids = main_model.generate(\n",
    "\t\tmax_new_tokens=100,\n",
    "\t\tdo_sample=True,\n",
    "\t\tnum_return_sequences=N\n",
    "\t\t)\n",
    "\t\n",
    "\ttext_samples = main_tokenizer.batch_decode(\n",
    "\t\tgenerated_ids,\n",
    "\t\tskip_special_tokens=True\n",
    "\t\t)\n",
    "\t\n",
    "\treward_model.to(device)\n",
    "\tinputs = reward_tokenizer(\n",
    "\t\ttext_samples,\n",
    "\t\ttruncation=True, padding=True, return_tensors='pt').to(device)\n",
    "\t\n",
    "\twith no_grad():\n",
    "\t\treward = reward_model(**inputs).logits[:, 0]\n",
    "\t\n",
    "\treturn text_samples[reward.argmax().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "cI5kuJ-Gb6Pn",
    "outputId": "b1ef6774-5a12-43d1-aa5f-50efedd96b8c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sigh. We all know how many things can suck in the first half: a crappy car chase, a movie that doesn\\'t know what can go wrong, a plot that takes ages to figure. It\\'s even worse than a \"good movie\". And there\\'s the fact that all this movie needed is a small cameo and maybe an extra script from a couple of Hollywood stars who play big characters. The plot lines make no sense and take over 90% of the movie. It makes you wonder what'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_with_reward_guidance(\n",
    "\tmain_model, main_tokenizer,\n",
    "\treward_model, reward_tokenizer,\n",
    "\tdevice=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NjQ40BRoH5f"
   },
   "source": [
    "# Stage 2: fine-tune the main model with RL\n",
    "\n",
    "\n",
    "Now, we will optimize GPT2 to produce negative IMDB movie reviews using the reward model you trained above.\n",
    "\n",
    "Unlike supervised fine-tuning, RL allows model to generate it's own sentences on each training step. Then, it calculates the reward of those specific sentences, and finally, updates the model to increase the probability of sentences with high reward.\n",
    "\n",
    "Thus, each RLHF consists of three stages: __Rollout__, __Evaluation__ and __Update__\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src='https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/gpt2_bert_training.png' width='600'>\n",
    "\n",
    "The update stage depends on the specific RL algorithm. We'll be using Proximal Policy Optimization, or [PPO](https://arxiv.org/abs/1707.06347), similarly to what was used for InstructGPT.\n",
    "\n",
    "Before we run those 3 stages, however, we need to create a dataset of \"queries\" - partial reviews in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119,
     "referenced_widgets": [
      "14bab8c0ec8f4b7ea41d4e74a31d280b",
      "c6d1254f73fc441897fb289661579fcc",
      "4cdf531ea3f545ffa6e0d0f68389f653",
      "a36ee9303657431093a8b33d713e9bd8",
      "daad181cf83044a5bc285f7925176464",
      "5482e18aa0564a50a7599875aa7e053f",
      "4ddb8f654c5a45e38c588cac213d0eb8",
      "f67d8a3f95074b01853851dd5b41cfa6",
      "b4b6c04b33e44613a3736505a4899d71",
      "f49a71e7d53342809dc887e254aa9d9f",
      "c52d8a32faa54cd9b7541441d3551ded",
      "5470a457a3e2466789ab2f3ca3289daa",
      "afb680bcb0fc4f4a94f04b5c086f77c6",
      "e76aa419c1864ab7906f58c09b53efb8",
      "0970efe0166f40b4a4355f474bfe182a",
      "64858c8a884d4e0792e0f6894015ead5",
      "a0134acdd9764db08cdb145a3b816eb3",
      "b07ef37f928646c7807471849cf6f71a",
      "5dab60bfa0f54cadbc163b4331732185",
      "a35d7bf116124060b525d12cd119ac5b",
      "305a6c274faa46beb46e8a2eff32d0fe",
      "5eec4a26a59c4b108bf576f21cad07f0"
     ]
    },
    "id": "jm5IUrer0xd_",
    "outputId": "2acab8d0-b1c1-4742-ec9b-95f8647d0526"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 25000/25000 [00:00<00:00, 368186.24 examples/s]\n",
      "Map:   0%|          | 0/24895 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 24895/24895 [00:17<00:00, 1446.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Note: this code is specific to IMDB; you will need to re-write it for other tasks\n",
    "imdb_for_rlhf = imdb.filter(lambda row: len(row['text']) > 200, batched=False)\n",
    "imdb_for_rlhf = imdb_for_rlhf.remove_columns(['label'])\n",
    "sample_length = trl.core.LengthSampler(2, 8)  # use the first 2-8 tokens as query\n",
    "\n",
    "def select_query_and_tokenize(sample):\n",
    "\tquery_ids = main_tokenizer.encode(sample[\"text\"])[: sample_length()]\n",
    "\tsample[\"query\"] = main_tokenizer.decode(query_ids)  # query is the only required column\n",
    "\tsample[\"input_ids\"] = query_ids  # to avoid re-tokenizing later\n",
    "\treturn sample  # we do not need the rest - it will be generated by the model\n",
    "\n",
    "imdb_for_rlhf = imdb_for_rlhf.map(select_query_and_tokenize, batched=False)\n",
    "imdb_for_rlhf.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3buACYV4QLJ"
   },
   "source": [
    "Finally, we move to RL training. In this tutorial, we'll train LoRA adapters and not the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nar1yXgl4KQa",
    "outputId": "6fcd294c-3151-4494-a62f-14e4989e4259"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\accelerate\\accelerator.py:439: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\transformers\\modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\trl\\models\\modeling_base.py:298: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n",
      "d:\\miniconda3\\envs\\nlp-vk-05\\lib\\site-packages\\peft\\tuners\\lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 125,620,225 || trainable%: 0.9390589771670923\n"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "peft_config = peft.LoraConfig(\n",
    "\ttask_type=peft.TaskType.CAUSAL_LM, r=32, lora_alpha=32, lora_dropout=0.0, inference_mode=False\n",
    ")\n",
    "\n",
    "# reload main model as AutoModelForCausalLMWithValueHead - with an extra head needed for PPO\n",
    "main_tokenizer = transformers.AutoTokenizer.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
    "main_tokenizer.pad_token = main_tokenizer.eos_token\n",
    "\n",
    "main_model = trl.AutoModelForCausalLMWithValueHead.from_pretrained(\"lvwerra/gpt2-imdb\", device_map=device)\n",
    "main_model = peft.get_peft_model(main_model, peft_config, adapter_name='default')\n",
    "main_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIQK5bcpCPZ6"
   },
   "source": [
    "Same as before, trl has a special type of trainer that minimize PPO-specific pseudo-loss. You can read more on this trainer [here](https://huggingface.co/docs/trl/main/en/ppo_trainer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "m4gvjdKbmeJf"
   },
   "outputs": [],
   "source": [
    "training_args = trl.PPOConfig(\n",
    "\tmodel_name=main_model.config._name_or_path,\n",
    "\tgradient_accumulation_steps=1,\n",
    "\tlearning_rate=1.41e-5,\n",
    "\tbatch_size=64,\n",
    "\tppo_epochs=4,                 # PPO performs this many updates per training batch\n",
    ")\n",
    "\n",
    "ppo_trainer = trl.PPOTrainer(\n",
    "\ttraining_args, model=main_model.model, tokenizer=main_tokenizer,\n",
    "\tdataset=imdb_for_rlhf, data_collator=lambda data: dict((key, [d[key] for d in data]) for key in data[0])\n",
    ")  # note: we pass main_model.model because PPOTrainer checks for one of several supported model types ...\n",
    "# ... main_model.model is a model with adapters, which is supported. main_model itself is a wrapper that is not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "90b8fdb8b9fb438f84c4f390683b0f3f",
      "af3a05a0c4ca49a385776d6c42427f8a",
      "ce8dea39d6b44be18e8f94392f94e7ce",
      "7b5922a62de94ee29cb6eefc9193190d",
      "0daa5834f1834dea9d5e40e6a3c4b376",
      "11f6a941d42f4692a53f94f36aca62d5",
      "6d693232f3c048ad944a47ee5742187c",
      "d833f6bf46c04b64acd1c04165c8f813",
      "0033ac842aab47e48f35151855112979",
      "60cbfb4df6da47d4b8b9624047b91cf2",
      "73db6e451d7c401d8b341c4352809e20"
     ]
    },
    "id": "eYr-w666-QfK",
    "outputId": "9dc19617-96cc-4ecc-d49f-dc8326a363e0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  2%|▏         | 1/50 [00:36<29:55, 36.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 0 ------------------------------\n",
      "rewards/mean:\t0.184813499\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t0.055444050\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t-0.421159863\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.000000000\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [01:13<29:38, 37.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 1 ------------------------------\n",
      "rewards/mean:\t0.408388615\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t0.161327419\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t-0.383026958\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.069897503\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [01:51<29:09, 37.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 2 ------------------------------\n",
      "rewards/mean:\t-0.085626602\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t0.087241213\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t-0.404487967\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.110206619\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [02:28<28:27, 37.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 3 ------------------------------\n",
      "rewards/mean:\t-0.300073624\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t-0.028953238\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t-0.477114022\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t0.616876841\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [03:04<27:38, 36.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 4 ------------------------------\n",
      "rewards/mean:\t-0.052922249\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t-0.036143941\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t-0.448472112\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t1.427753329\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [03:40<26:49, 36.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 5 ------------------------------\n",
      "rewards/mean:\t0.346815109\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t0.078743774\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t-0.303512335\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.009539127\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [04:16<25:57, 36.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 6 ------------------------------\n",
      "rewards/mean:\t1.154198647\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t0.401380236\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t-0.084377229\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t2.812479496\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [04:55<25:59, 37.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 7 ------------------------------\n",
      "rewards/mean:\t0.881374359\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t0.545378473\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t-0.006371658\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t3.780475616\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [05:33<25:40, 37.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 8 ------------------------------\n",
      "rewards/mean:\t0.463375092\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t0.520777458\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t-0.021193445\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.312567711\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [06:10<24:54, 37.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 9 ------------------------------\n",
      "rewards/mean:\t0.787911415\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t0.600917645\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.078692898\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t5.806142330\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [06:47<24:07, 37.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 10 ------------------------------\n",
      "rewards/mean:\t0.334899068\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t0.521112072\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.039771907\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t6.280764580\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [07:24<23:31, 37.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 11 ------------------------------\n",
      "rewards/mean:\t1.217950821\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t0.730163697\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.096000120\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.058010101\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [08:01<22:50, 37.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 12 ------------------------------\n",
      "rewards/mean:\t1.296674728\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t0.900117006\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.181105554\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.411641121\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [08:38<22:12, 37.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 13 ------------------------------\n",
      "rewards/mean:\t1.005467534\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t0.931722165\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.207273066\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.628366947\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [09:14<21:21, 36.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 14 ------------------------------\n",
      "rewards/mean:\t1.545151711\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t1.115751028\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.357464492\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.351938248\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [09:50<20:42, 36.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 15 ------------------------------\n",
      "rewards/mean:\t1.138256073\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t1.122502542\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.235509828\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.184993744\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [10:26<20:05, 36.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 16 ------------------------------\n",
      "rewards/mean:\t0.975373983\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t1.078363974\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.233827069\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.465982437\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [11:07<20:04, 37.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 17 ------------------------------\n",
      "rewards/mean:\t1.262933731\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t1.133734901\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.322418392\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.753006458\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [11:44<19:22, 37.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 18 ------------------------------\n",
      "rewards/mean:\t1.718078613\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t1.309038015\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.380621910\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.846158981\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [12:20<18:32, 37.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 19 ------------------------------\n",
      "rewards/mean:\t1.872340202\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t1.478028671\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.506123424\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.042571068\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [12:57<17:55, 37.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 20 ------------------------------\n",
      "rewards/mean:\t1.371367216\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t1.446030235\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.545753360\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t7.333931446\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [13:33<17:06, 36.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 21 ------------------------------\n",
      "rewards/mean:\t2.155541420\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t1.658883590\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.674237370\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.028120041\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [14:09<16:25, 36.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 22 ------------------------------\n",
      "rewards/mean:\t2.335096359\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t1.861747421\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.852169514\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.251592636\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [14:44<15:42, 36.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 23 ------------------------------\n",
      "rewards/mean:\t2.365423203\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t2.012850155\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.946076751\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.439592361\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [15:19<14:53, 35.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 24 ------------------------------\n",
      "rewards/mean:\t2.512689114\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t2.162801843\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.128260136\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t9.519665718\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [15:53<14:04, 35.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 25 ------------------------------\n",
      "rewards/mean:\t2.036746979\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t2.124985384\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.923455477\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.514331818\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [16:29<13:32, 35.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 26 ------------------------------\n",
      "rewards/mean:\t2.507891655\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t2.239857265\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.984710634\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.754348755\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [17:08<13:26, 36.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 27 ------------------------------\n",
      "rewards/mean:\t2.476023436\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t2.310707116\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.041142941\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.956239700\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [17:43<12:36, 36.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 28 ------------------------------\n",
      "rewards/mean:\t2.333978176\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t2.317688434\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t0.968672752\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t8.873231888\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [18:19<11:59, 35.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 29 ------------------------------\n",
      "rewards/mean:\t2.526615143\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t2.380366447\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.252208948\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.057155609\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [18:54<11:18, 35.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 30 ------------------------------\n",
      "rewards/mean:\t3.062608957\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t2.585039200\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.464666605\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t11.300116539\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [19:30<10:43, 35.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 31 ------------------------------\n",
      "rewards/mean:\t2.322861671\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t2.506385941\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.462383986\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.169392586\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [20:05<10:06, 35.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 32 ------------------------------\n",
      "rewards/mean:\t2.927136421\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t2.632611085\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.551594496\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.427952766\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [20:38<09:19, 34.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 33 ------------------------------\n",
      "rewards/mean:\t2.977771759\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t2.736159287\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.585646868\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t10.313037872\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [21:12<08:37, 34.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 34 ------------------------------\n",
      "rewards/mean:\t2.639501572\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t2.707161973\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.515711546\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.010259628\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [21:46<08:01, 34.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 35 ------------------------------\n",
      "rewards/mean:\t3.018028259\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t2.800421859\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.647488356\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t14.720853806\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [22:18<07:16, 33.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 36 ------------------------------\n",
      "rewards/mean:\t3.111459732\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t2.893733221\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.681805968\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t13.077782631\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [22:53<06:48, 34.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 37 ------------------------------\n",
      "rewards/mean:\t3.252457619\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t3.001350540\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.781169415\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t13.318946838\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [23:26<06:13, 33.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 38 ------------------------------\n",
      "rewards/mean:\t3.181152344\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t3.055291081\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.850888252\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t14.168682098\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [24:06<05:57, 35.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 39 ------------------------------\n",
      "rewards/mean:\t3.101176262\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t3.069056635\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.878402948\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t13.739465714\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [24:46<05:31, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 40 ------------------------------\n",
      "rewards/mean:\t3.492445946\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t3.196073429\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.861994147\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t14.708334923\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [25:23<04:55, 37.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 41 ------------------------------\n",
      "rewards/mean:\t3.222915173\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t3.204125952\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.902049065\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t14.603998184\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [26:02<04:23, 37.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 42 ------------------------------\n",
      "rewards/mean:\t3.172294617\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t3.194576551\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.869485855\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t13.483745575\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [26:38<03:42, 37.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 43 ------------------------------\n",
      "rewards/mean:\t2.957304955\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t3.123395072\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.709170938\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t14.789612770\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [27:10<02:57, 35.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 44 ------------------------------\n",
      "rewards/mean:\t3.133522987\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t3.126433447\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.681799173\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.233568192\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [27:43<02:19, 34.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 45 ------------------------------\n",
      "rewards/mean:\t3.211109161\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t3.151836161\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.926575422\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.679422379\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [28:19<01:45, 35.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 46 ------------------------------\n",
      "rewards/mean:\t3.836676121\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t3.357288149\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t2.115183830\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t14.401512146\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [28:58<01:12, 36.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 47 ------------------------------\n",
      "rewards/mean:\t3.096509933\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t3.279054684\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.950516462\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t12.884355545\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [29:32<00:35, 35.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 48 ------------------------------\n",
      "rewards/mean:\t3.509265184\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t3.348117834\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t2.106852770\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t15.362712860\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [30:03<00:00, 36.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ STEP 49 ------------------------------\n",
      "rewards/mean:\t3.398410797\t<---- average reward over this batch (higher=better, noisy)\n",
      "rewards/moving_avg:\t3.363205723\t<---- moving average reward (higher=better, less noisy)\n",
      "ppo/returns/mean:\t1.960811734\t<---- model-estimated average discounted reward\n",
      "objective/kl:\t15.823799133\t<---- how far we are from the original model (regularizer)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "max_steps = 50   # can be insufficient for some tasks - watch your learning curves\n",
    "generation_kwargs = dict(\n",
    "\t\tmin_length=-1, max_new_tokens=128, do_sample=True, top_k=0, top_p=1.0, pad_token_id=main_tokenizer.eos_token_id)\n",
    "#                                  ^-- task-specific parameter!\n",
    "\n",
    "average_reward = 0\n",
    "gamma = 0.7\n",
    "\n",
    "with tqdm(enumerate(ppo_trainer.dataloader), total=max_steps) as progressbar:\n",
    "\t# note: ppo_trainer.dataloader is just a regular dataloader of queries, no RL-specific magic :)\n",
    "\tfor epoch, batch in progressbar:\n",
    "\t\tif epoch >= max_steps:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t# Rollout stage: generate continuations from batch queries using main_model\n",
    "\t\tresponse_tensors = ppo_trainer.generate(batch['input_ids'], **generation_kwargs)\n",
    "\t\t# ^-- list of tensors of token ids from main model tokenizer\n",
    "\n",
    "\t\t# de-tokenize responses to strings (since reward model uses a different tokenizer)\n",
    "\t\tbatch[\"response\"] = [main_tokenizer.decode(response.squeeze()) for response in response_tensors]\n",
    "\t\t# note: response_tensors already contain query tokens, so we don't need to add queries manually.\n",
    "\t\t# This may not be true for other tasks: check this manually by viewing batch[\"response\"] and batch[\"query\"]\n",
    "\n",
    "\n",
    "\t\t# Evaluation stage - rewards for batch['response']\n",
    "\t\trewards = compute_reward(reward_model, reward_tokenizer, batch[\"response\"], device=device)\n",
    "\n",
    "\t\t# Update stage\n",
    "\t\tstats = ppo_trainer.step(batch['input_ids'], response_tensors, list(rewards.split(1)))\n",
    "\t\tstats['rewards/mean'] = rewards.mean().item()   # compute mean rewards for batch\n",
    "\t\taverage_reward = gamma * average_reward + (1 - gamma) * stats['rewards/mean']\n",
    "\n",
    "\t\tprint(\"-\" * 30, 'STEP', epoch, '-' * 30)\n",
    "\t\tprint(f'rewards/mean:\\t{stats[\"rewards/mean\"]:.9f}\\t<---- average reward over this batch (higher=better, noisy)')\n",
    "\t\tprint(f'rewards/moving_avg:\\t{average_reward:.9f}\\t<---- moving average reward (higher=better, less noisy)')\n",
    "\t\tprint(f'ppo/returns/mean:\\t{stats[\"ppo/returns/mean\"]:.9f}\\t<---- model-estimated average discounted reward')\n",
    "\t\tprint(f'objective/kl:\\t{stats[\"objective/kl\"]:.9f}\\t<---- how far we are from the original model (regularizer)')\n",
    "\t\tprint()\n",
    "\n",
    "\t\tppo_trainer.log_stats(stats, batch, list(rewards.split(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "Xoq0vpCxx5IZ",
    "outputId": "a01352f3-5b33-4a1a-9391-09abc4003868"
   },
   "outputs": [],
   "source": [
    "assert average_reward > 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now test your PPO model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "_RV1n2vgiZP_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: The movie was made by not being highschooled with the teen zombie flick, it actually made you wish you had been in it, as though it were eye candy...with a bit of self pity and misreading the art.<|endoftext|>\n",
      "Sample: The movie was a remake of The Matrix - but IMDb has static images instead of static images: the anti-film take it out is junk. There is a sign published saying there is bad remake.<|endoftext|>\n",
      "Sample: The movie was just called'movie', 'poster'. And the direction. o laurels make it look bad. The sales people could not have sell it. The 80's movies were just wooden from what I have seen in the past 8 years, the only movie that ever made it was called 'Do Not Miss Me'. And that movie with Hillary cyber scam exposed to people and corporations' is nothing comparison, and that movie is awful. This is the worst movie movie in garbage every time.<|endoftext|>\n",
      "Sample: The movie was better than the film. It got everything superfluier than the film. For it gets to the very depressing story of irony, and is pathetic. This film was even worse.<|endoftext|>\n",
      "Sample: The movie was cheap as waste. <br /><br />This movie poorly made - the only thing better than this doc stringed was the trailer.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "inputs = [main_tokenizer.encode(\"The movie was\", return_tensors='pt').to(device)[0] for i in range(5)]\n",
    "\n",
    "response_tensors = ppo_trainer.generate(inputs, **generation_kwargs)\n",
    "batch[\"response\"] = [main_tokenizer.decode(response.squeeze()) for response in response_tensors]\n",
    "for sample in batch[\"response\"]:\n",
    "\tprint('Sample: {}'.format(sample))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp-vk-05",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0033ac842aab47e48f35151855112979": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "015524419b8a4a21bec9ca2462a1bdae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3bb5755a3f44b519c4203430846e3d2",
      "max": 390,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_62e010f8c15d402aba1e045de8e8a50b",
      "value": 390
     }
    },
    "0970efe0166f40b4a4355f474bfe182a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_305a6c274faa46beb46e8a2eff32d0fe",
      "placeholder": "​",
      "style": "IPY_MODEL_5eec4a26a59c4b108bf576f21cad07f0",
      "value": " 24895/24895 [00:31&lt;00:00, 819.84 examples/s]"
     }
    },
    "0daa5834f1834dea9d5e40e6a3c4b376": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11b0022cfbaa488db010a2f22a17b7cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3fd865cd993242cdad016d9794a75f6e",
      "placeholder": "​",
      "style": "IPY_MODEL_f986e194b4944ead94190797f28015fe",
      "value": ""
     }
    },
    "11f6a941d42f4692a53f94f36aca62d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14bab8c0ec8f4b7ea41d4e74a31d280b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c6d1254f73fc441897fb289661579fcc",
       "IPY_MODEL_4cdf531ea3f545ffa6e0d0f68389f653",
       "IPY_MODEL_a36ee9303657431093a8b33d713e9bd8"
      ],
      "layout": "IPY_MODEL_daad181cf83044a5bc285f7925176464"
     }
    },
    "305a6c274faa46beb46e8a2eff32d0fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "351c280e3e0644e08478893632ca878c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_579edffceded4184ab0009cef5e8d87e",
      "placeholder": "​",
      "style": "IPY_MODEL_83eac7e3fc554ea09c2a41452cad2cde",
      "value": " 391/? [02:18&lt;00:00,  3.09it/s]"
     }
    },
    "3fd865cd993242cdad016d9794a75f6e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cdf531ea3f545ffa6e0d0f68389f653": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f67d8a3f95074b01853851dd5b41cfa6",
      "max": 25000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b4b6c04b33e44613a3736505a4899d71",
      "value": 25000
     }
    },
    "4ddb8f654c5a45e38c588cac213d0eb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5470a457a3e2466789ab2f3ca3289daa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_afb680bcb0fc4f4a94f04b5c086f77c6",
       "IPY_MODEL_e76aa419c1864ab7906f58c09b53efb8",
       "IPY_MODEL_0970efe0166f40b4a4355f474bfe182a"
      ],
      "layout": "IPY_MODEL_64858c8a884d4e0792e0f6894015ead5"
     }
    },
    "5482e18aa0564a50a7599875aa7e053f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "579edffceded4184ab0009cef5e8d87e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5dab60bfa0f54cadbc163b4331732185": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5eec4a26a59c4b108bf576f21cad07f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60cbfb4df6da47d4b8b9624047b91cf2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62e010f8c15d402aba1e045de8e8a50b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "64858c8a884d4e0792e0f6894015ead5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d693232f3c048ad944a47ee5742187c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73db6e451d7c401d8b341c4352809e20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b5922a62de94ee29cb6eefc9193190d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60cbfb4df6da47d4b8b9624047b91cf2",
      "placeholder": "​",
      "style": "IPY_MODEL_73db6e451d7c401d8b341c4352809e20",
      "value": " 30/50 [25:39&lt;15:10, 45.54s/it]"
     }
    },
    "83eac7e3fc554ea09c2a41452cad2cde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90b8fdb8b9fb438f84c4f390683b0f3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_af3a05a0c4ca49a385776d6c42427f8a",
       "IPY_MODEL_ce8dea39d6b44be18e8f94392f94e7ce",
       "IPY_MODEL_7b5922a62de94ee29cb6eefc9193190d"
      ],
      "layout": "IPY_MODEL_0daa5834f1834dea9d5e40e6a3c4b376"
     }
    },
    "9123a18088fb454d915218f37f72a790": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0134acdd9764db08cdb145a3b816eb3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a35d7bf116124060b525d12cd119ac5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a36ee9303657431093a8b33d713e9bd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f49a71e7d53342809dc887e254aa9d9f",
      "placeholder": "​",
      "style": "IPY_MODEL_c52d8a32faa54cd9b7541441d3551ded",
      "value": " 25000/25000 [00:00&lt;00:00, 9679.51 examples/s]"
     }
    },
    "af3a05a0c4ca49a385776d6c42427f8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11f6a941d42f4692a53f94f36aca62d5",
      "placeholder": "​",
      "style": "IPY_MODEL_6d693232f3c048ad944a47ee5742187c",
      "value": " 60%"
     }
    },
    "afb680bcb0fc4f4a94f04b5c086f77c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0134acdd9764db08cdb145a3b816eb3",
      "placeholder": "​",
      "style": "IPY_MODEL_b07ef37f928646c7807471849cf6f71a",
      "value": "Map: 100%"
     }
    },
    "b07ef37f928646c7807471849cf6f71a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4b6c04b33e44613a3736505a4899d71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c3bb5755a3f44b519c4203430846e3d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c52d8a32faa54cd9b7541441d3551ded": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6d1254f73fc441897fb289661579fcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5482e18aa0564a50a7599875aa7e053f",
      "placeholder": "​",
      "style": "IPY_MODEL_4ddb8f654c5a45e38c588cac213d0eb8",
      "value": "Filter: 100%"
     }
    },
    "ce8dea39d6b44be18e8f94392f94e7ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d833f6bf46c04b64acd1c04165c8f813",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0033ac842aab47e48f35151855112979",
      "value": 30
     }
    },
    "d833f6bf46c04b64acd1c04165c8f813": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daad181cf83044a5bc285f7925176464": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e76aa419c1864ab7906f58c09b53efb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dab60bfa0f54cadbc163b4331732185",
      "max": 24895,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a35d7bf116124060b525d12cd119ac5b",
      "value": 24895
     }
    },
    "f49a71e7d53342809dc887e254aa9d9f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f67d8a3f95074b01853851dd5b41cfa6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f70928bb66b24415ad03f49633775e64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_11b0022cfbaa488db010a2f22a17b7cc",
       "IPY_MODEL_015524419b8a4a21bec9ca2462a1bdae",
       "IPY_MODEL_351c280e3e0644e08478893632ca878c"
      ],
      "layout": "IPY_MODEL_9123a18088fb454d915218f37f72a790"
     }
    },
    "f986e194b4944ead94190797f28015fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
